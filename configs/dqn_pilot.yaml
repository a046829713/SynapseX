# @package _global_

defaults:
  - _self_
  - model: small


keyword:
  algorithm: "DQN"
  model: "HOPE"

# 2. 訓練設定 (RL Hyperparameters)
train:
  device: "cuda:0"         # 指定訓練設備
  steps: 100000            # 總訓練步數
  batch_size: 64           # 每次從 Replay Buffer 採樣的大小
  buffer_size: 50000       # Replay Buffer 的最大容量
  
  # --- DQN 參數 ---
  bars_count : 300
  gamma: 0.99              # 折扣因子 (Discount Factor)
  target_update_freq: 1000 # 多久更新一次 Target Network (步數)
  
  # --- Epsilon-Greedy (探索策略) ---
  epsilon_start: 1.0       # 一開始 100% 隨機探索
  epsilon_end: 0.05        # 最後保留 5% 隨機探索
  epsilon_decay: 20000     # 經過多少步衰減到接近最小值
  epsilon_steps_factor : 0 # 函數產生


  # strategy
  model_default_commission_perc_traing : 0.0045
  default_slippage : 0.0025
  n_steps : 1000
  win_payoff_weight : 1.0

  # --- 存檔設定 ---
  checkpoint:
    enable: true
    save_interval: 5000    # 每 5000 步存一個檔
    saves_tag: "saves"
    saves_path: ""         # 函數產生

  train_data_file_path : ["Brain","simulation","train_data"]
  unique_symbols : [] # 函數產生 (訓練資料表)

# 3. 優化器設定
optim:
  type: "adamw"
  lr: 3e-4                 # 學習率 (通常 RL 用 1e-4 到 3e-4)
  weight_decay: 0.01
  betas: [0.9, 0.95]

# 4. 記錄設定 (WandB 或本地 log)
logging:
  project: "hope-dqn-darkchess"
  name: "pilot-run-001"
  log_interval: 100        # 每 100 步印一次 log