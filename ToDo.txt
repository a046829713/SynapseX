1.1. 演算法概念說明
1.1 階段 A：選股 (Higher-level Policy)
目標：從大量股票池 (例如 100 檔) 中挑選出有潛力的子集 (例如 10 檔)。
週期：可以在較長的時間尺度（如「每週/每月」）執行一次，或在你定義的固定頻率觸發。
環境狀態：
包含所有股票的財務、技術、新聞因子等資訊 (日級/週級資料)
當前持有的標的物（若已有）。
動作：
選擇一組標的物（或分配資產權重），例如一個長度為 100 的二元向量 (1=選擇, 0=不選) 或連續值代表權重。
獎勵：
可以直接用「隨後在交易階段的總收益」作為獎勵，或使用分層設計，在高階先估計一個長期預期回報。
1.2 階段 B：交易 (Lower-level Policy)
目標：對已選出的標的物進行更頻繁的買/賣/持有等操作。
週期：分鐘級別、或更短時間週期（視資料而定）。
環境狀態：
僅包含在階段 A 挑選出的子集股票的分鐘線資料、當前持倉、可用資金等。
動作：
對每支被選的股票下單（買/賣/平倉），或決定持倉比例。
獎勵：
分鐘或小時級別的收益、或在一個交易日結束後的日度收益。
1.3 分層/多階段串接
整體流程：

在某個時間點（例如每週），高階策略 (階段 A) 針對全市場做出「選股」動作。
接下來的一週 (或幾天) 內，低階策略 (階段 B) 只針對已選出的股票，在分鐘級資料上執行強化學習交易。
一週（或固定週期）後，根據整體收益或風險調整後收益，回饋給高階策略，更新階段 A 的 Policy。
重複此週期。
Policy Gradient 更新：

可以在階段 A 與階段 B 各自維護一組參數 
𝜃
𝐴
θ 
A
​
  與 
𝜃
𝐵
θ 
B
​
 ，分別透過收集軌跡 (trajectories) 來計算梯度並更新。
若要在高階策略中回饋「低階交易」後的收益，需在高階週期結束時，再計算對 
𝜃
𝐴
θ 
A
​
  的梯度。
